{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ddede9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee4c21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"StudentPerformance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Study_Hours_per_Week', 'Attendance_Rate', 'Internal_Scores']].values\n",
    "y = np.where(df['Final_Exam_Score'] >= 50, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ed7f520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in y: [1]\n"
     ]
    }
   ],
   "source": [
    "# Check if both classes exist\n",
    "unique_classes = np.unique(y)\n",
    "print(f\"Classes in y: {unique_classes}\")\n",
    "\n",
    "# If only one class, use median instead of fixed threshold\n",
    "if len(unique_classes) == 1:\n",
    "    y = np.where(df['Final_Exam_Score'] >= df['Final_Exam_Score'].median(), 1, 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5cabbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "920e0a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = np.sum((y_pred == 1) & (y_test == 1))\n",
    "TN = np.sum((y_pred == 0) & (y_test == 0))\n",
    "FP = np.sum((y_pred == 1) & (y_test == 0))\n",
    "FN = np.sum((y_pred == 0) & (y_test == 1))\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8d1e7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Manual Evaluation Metrics =====\n",
      "True Positives (TP): 62\n",
      "True Negatives (TN): 57\n",
      "False Positives (FP): 14\n",
      "False Negatives (FN): 9\n",
      "\n",
      "Precision: 0.82\n",
      "Recall:    0.87\n",
      "F1-Score:  0.84\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== Manual Evaluation Metrics =====\")\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\\n\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall:    {recall:.2f}\")\n",
    "print(f\"F1-Score:  {f1:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
